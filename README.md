# ğŸ§  AI Learning Gap Assistant

### Bloom-Aware Conceptual Tutor with Reflective Analytics & Learning Refinement

An AI-powered conceptual learning assistant that helps students bridge learning gaps through:

* Bloom Learning Levels
* Progressive refinement explanations
* Confidence-based reflection
* Revision-priority insights
* Structured learning summaries
* Lightweight learning analytics dashboard

Instead of only *answering questions*, the assistant supports:

âœ” understanding
âœ” reflection
âœ” progressive clarity
âœ” learning reinforcement

This project aligns with **SDG-4 â€” Quality Education** by enabling reflective, feedback-driven learning.

---

## ğŸ“Œ Project Overview

Traditional AI chat responses provide answers â€” but they do not:

âœ˜ assess learning depth
âœ˜ support cognitive learning levels
âœ˜ provide refinement-based explanations
âœ˜ capture confidence signals
âœ˜ identify weak topics

This system shifts AI from:

> â€œAnswer providerâ€ â†’ â€œLearning facilitatorâ€

by encouraging:

* conceptual clarity
* repeated scaffolding
* reflective thinking
* insight-driven revision

---

# ğŸ§© Visual Learning Flow

### **How a Learning Session Works**

```mermaid
flowchart TD

A[Student Asks Question] --> B[AI Generates Main Explanation (v1)]
B --> C{Student Feedback?}

C -->|Needs Simpler Form| D[Refinement Mode â€” Simpler Explanation (v2)]
C -->|Needs More Examples| E[Refinement Mode â€” Example-Based Explanation (v2)]
C -->|Understood| F[Confidence Recorded]

D --> G[Confidence Check]
E --> G[Confidence Check]

G --> H[Confidence Trend Logged]
H --> I[Revision Priority Evaluated]

I --> J[Learning Analytics Dashboard]
J --> K[Reflection-Based Learning Summary Export]
```

The system treats learning as a **process**, not a single response.

---

# ğŸš Bloom Learning Level Support

Learner selects cognitive depth:

| Level      | Meaning                | Focus                   |
| ---------- | ---------------------- | ----------------------- |
| Remember   | Recall / definition    | Basic concepts          |
| Understand | Concept clarity        | Meaning & explanation   |
| Apply      | Real-world use         | Context & examples      |
| Analyze    | Reasoning & comparison | Insight & relationships |

This encourages progression from **recall â†’ understanding â†’ application â†’ analysis**.

---

# ğŸ” Progressive Refinement Loop

Students can request:

ğŸ§© *Explain in simpler words*
ğŸ“Œ *Give more real-world examples*

The assistant then:

âœ” preserves version-1 explanation
âœ” adds refinement versions
âœ” logs learning attempt history

Examples:

* v1 â€” main conceptual explanation
* v2 â€” simpler scaffolded form
* v3 â€” example-driven understanding

All versions are saved in the session log.

---

# ğŸ‘ Confidence-Based Reflection

After each explanation, learner provides:

* High confidence
* Medium confidence
* Low confidence

Confidence signals enable:

âœ” metacognitive awareness
âœ” learning reflection
âœ” revision recommendations

Confidence trends are factored into:

* analytics dashboard
* learning summary export
* revision-priority evaluation

---

# ğŸ“Š Learning Analytics Dashboard

The dashboard surfaces:

| Metric                       | Meaning                          |
| ---------------------------- | -------------------------------- |
| Total Questions              | Cognitive engagement             |
| Refinement Attempts          | Struggle / clarity signals       |
| Avg Refinements per Question | Concept difficulty trend         |
| Bloom Level Distribution     | Thinking depth mapping           |
| Confidence Trend             | Learning self-reflection pattern |

This makes learning **insight-oriented** rather than chat-based.

---

# ğŸ“ Reflection-Based Learning Summary (Exportable)

Each session generates a structured report:

Includes:

âœ” session topic
âœ” questions asked
âœ” Bloom level
âœ” refinement attempts
âœ” multiple explanation versions
âœ” confidence-trend insight
âœ” revision-priority tag

Export options:

* TXT
* PDF *(wrapped & multi-page safe)*
* JSON session log *(research-friendly)*

---

# ğŸ— System Architecture Flow

```mermaid
flowchart LR

A[Student Input] 
--> B[Chat Context Memory]
--> C[Groq LLM Response Engine]
--> D[Primary Answer v1]

D --> E{Refinement Trigger?}
E -->|Simpler| F[Refinement v2 â€” Simplified]
E -->|Examples| G[Refinement v2 â€” Example-Based]

F --> H[Chat History]
G --> H[Chat History]

H --> I[Meta Log Store (JSON)]
I --> J[Analytics Engine]
J --> K[Reflection Summary Export]
```

The system treats each doubt as a **learning event**.

---

# ğŸ§© Key Features

âœ” Bloom-aware conceptual answering
âœ” Multi-version refinement explanations
âœ” Confidence-based learning reflection
âœ” Revision-priority inference
âœ” Persistent session logging
âœ” Downloadable learning summaries
âœ” JSON logs for research / ML use
âœ” Modern Streamlit Indigo dashboard theme

All features are implemented in `app.py` 

---

# ğŸ— Tech Stack

* Python
* Streamlit
* Groq API (LLM inference)
* ReportLab (PDF export)
* JSON-based session persistence

---

# ğŸ’» Run Locally

### Create venv

```
python -m venv .venv
```

Activate:

```
.venv\Scripts\activate   # Windows
```

### Install dependencies

```
pip install -r requirements.txt
```

### Create `.env`

```
GROQ_API_KEY=your_api_key
```

### Run app

```
streamlit run app.py
```

---

# â˜ Deployment (Streamlit Cloud)

Add secret:

```
GROQ_API_KEY="your_api_key"
```

Deploy and run.

---

# ğŸ“ Pedagogical Value

Supports:

âœ” conceptual understanding
âœ” scaffolding-based clarity
âœ” reflective self-assessment
âœ” insight-driven revision
âœ” SDG-4 learning equity goals

This system is built as a **learning facilitator â€” not a shortcut answer tool**.

---

# ğŸ›  Future Enhancements

* per-question confidence intelligence
* learner progress timeline
* recommendation engine for revision topics
* teacher / mentor dashboard
* concept difficulty heat-mapping

---

# ğŸ‘¤ Author

**Sushant Adhav**
CSRBOX â€” IBM SkillsBuild Applied AI Internship (2025)
